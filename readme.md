# ğŸ§  Local Multi-Document RAG Assistant (100% Free)

A fully local Retrieval-Augmented Generation (RAG) system built using:

- ğŸ¦™ Ollama (Llama 3)
- ğŸ¤— HuggingFace Embeddings (all-MiniLM-L6-v2)
- ğŸ“¦ Chroma Vector Database (persistent)
- ğŸ”— LangChain (LCEL pipeline)
- ğŸ¨ Gradio UI

This project supports **multi-PDF ingestion** and **source-aware responses** â€” all without any API costs.

---

## ğŸš€ Features

- âœ… Fully local LLM (llama3 via Ollama)
- âœ… Local embedding model (no API usage)
- âœ… Persistent vector database (Chroma)
- âœ… Multi-document PDF ingestion
- âœ… Source file attribution in responses
- âœ… Modular architecture (retriever + LLM separated)
- âœ… Zero cloud dependency
- âœ… GitHub-safe project structure

---

## ğŸ— Architecture

User Question
â†“
Retriever (Chroma)
â†“
Top Relevant Chunks (from multiple PDFs)
â†“
Prompt Construction
â†“
Llama3 via Ollama
â†“
Grounded Answer + Source Files

This design cleanly separates:

- Retrieval layer
- Prompt construction layer
- Generation layer

---

## ğŸ“‚ Project Structure

local-rag/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ rag/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ retriever.py
â”‚   â””â”€â”€ chain.py
â”‚
â”œâ”€â”€ data/           # Add your PDFs here (not tracked in Git)
â”œâ”€â”€ vectorstore/    # Auto-generated vector DB (not tracked)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## ğŸ“„ Adding Documents

This repository does not include sample PDFs.

To add your own documents:

1. Create a folder named `data/`
2. Add one or more PDF files inside:
data/
resume.pdf
architecture.pdf
policy.pdf
3. Run ingestion:
python rag/ingest.py
4. Launch the app:

---

## âš™ï¸ Installation

### 1ï¸âƒ£ Install Ollama

Download and install Ollama from:

https://ollama.com

Start Ollama:
ollama serve

Pull Llama3 model:
ollama pull llama3

---

### 2ï¸âƒ£ Install Python Dependencies

Create virtual environment:
python -m venv venv
source venv/bin/activate

Install requirements:
pip install -r requirements.txt

---

## ğŸ”’ Git Hygiene

The following directories are excluded from version control:

- `data/`
- `vectorstore/`
- `venv/`
- `__pycache__/`

This ensures the repository remains lightweight and reproducible.

---

## ğŸ§  How It Works

1. PDFs are loaded and split into semantic chunks
2. Each chunk is embedded using MiniLM
3. Embeddings are stored in Chroma
4. User query is embedded
5. Most relevant chunks are retrieved
6. Retrieved context + question are passed to Llama3
7. Model generates a grounded answer

---

## ğŸ›  Technical Highlights

- Uses LangChain's modern LCEL pipeline (Runnable-based architecture)
- Uses cosine similarity for semantic retrieval
- Embeddings are 384-dimensional vectors
- Fully offline and local inference
- Supports multi-document search

---

## ğŸš€ Future Improvements

- ğŸ”„ Conversational session memory
- ğŸ“‘ Document-level filtering
- âš¡ Streaming responses
- ğŸŒ FastAPI backend + React frontend
- ğŸ” Model switching abstraction
- ğŸ³ Dockerization
- â˜ï¸ Optional cloud deployment

---

## ğŸ“Œ Why This Project?

Most RAG projects rely on paid APIs.

This project demonstrates:

- How to build a fully local RAG pipeline
- How vector databases work internally
- How retrieval and generation layers interact
- Clean software architecture practices

---

## ğŸ“œ License

MIT License